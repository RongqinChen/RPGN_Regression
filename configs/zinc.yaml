batch_size: 64
num_workers: 12
lr: 1e-3
min_lr: 1e-6
l2_wd: 1e-5
num_epochs: 2000
num_warmup_epochs: 50
emb_channels: 16
hidden_channels: 64
num_layers: 10
mlp_depth: 2
norm_type: Batch
drop_prob: 0.0
graph_pool: mean
jumping_knowledge: last
task_type: graph_regression
num_task: 1
runs: 10
