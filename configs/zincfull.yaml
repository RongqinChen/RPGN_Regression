batch_size: 128
num_workers: 6
lr: 1e-3
min_lr: 1e-6
l2_wd: 1e-5
num_epochs: 2000
num_warmup_epochs: 50
emb_channels: 16
hidden_channels: 88
num_layers: 10
mlp_depth: 1
norm_type: Batch
drop_prob: 0.0
graph_pool: mean
jumping_knowledge: last
task_type: graph_regression
num_task: 1
runs: 10
pe_method: bern_mixed_sym2
pe_power: 16
